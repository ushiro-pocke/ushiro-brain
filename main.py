from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from sudachipy import dictionary, tokenizer
import random
import re
import csv
import os

app = FastAPI()

# --- CORSè¨­å®š ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ğŸ§  è§£æã‚¨ãƒ³ã‚¸ãƒ³ã®æº–å‚™ ---
# ã‚¨ãƒ©ãƒ¼ãŒå‡ºã«ãã„ã‚ˆã†ã€ä¸€ç•ªã‚·ãƒ³ãƒ—ãƒ«ãªè¨˜è¿°ã«å¤‰ãˆã¾ã—ãŸ
tokenizer_obj = dictionary.Dictionary().create()
mode = tokenizer.Tokenizer.SplitMode.C 

# --- ğŸ“š è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ ---
NOUN_DICT = {}

if os.path.exists("dict.csv"):
    with open("dict.csv", mode="r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2:
                key = row[0]
                candidates = row[1:]
                NOUN_DICT[key] = [c for c in candidates if c.strip()]

# --- ğŸ—£ ãƒ•ã‚£ãƒ©ãƒ¼ï¼ˆãƒã‚¤ã‚ºï¼‰ ---
FILLERS = ["ãˆãƒ¼ã£ã¨ã€", "ãªã‚“ã‹ã€", "æ­£ç›´ã€", "ã¶ã£ã¡ã‚ƒã‘ã€", "ã¦ã„ã†ã‹ã€", "å®Ÿã¯ã€"]

# --- ğŸ”š æ–‡æœ«è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ ---
ENDING_PATTERNS = [
    (r"ã§ã™ã€‚$", ["ã§ã™ã­ã€‚", "ã§ã™ã‚ˆã€‚", "ãªã‚“ã§ã™ã€‚", "ã ã­ã€‚"]),
    (r"ã¾ã™ã€‚$", ["ã¾ã™ã­ã€‚", "ã¾ã™ã‚ˆã€‚", "ã¡ã‚ƒã†ã‹ã‚‚ã€‚", "ã¾ã™ã€œã€‚"]),
    (r"ã§ã‚ã‚‹ã€‚$", ["ã§ã™ã€‚", "ã ã­ã€‚", "ãªã‚“ã ã‚ˆã­ã€‚"]),
]

# --- ğŸ¥ æ–‡æ³•æ•´å½¢ãƒ«ãƒ¼ãƒ« ---
GRAMMAR_FIXES = [
    (r"ã“ã¨(ã™ã‚‹|ã—ã¾ã™|ã—ãŸ|ã—ã¦)", r"ã“ã¨ã«\1"), 
    (r"([ã†ãã™ã¤ã¬ã‚€ã‚‹])ã™ã‚‹", r"\1"),
    (r"ã†ãŸ", r"ã£ãŸ"), (r"ã¤ãŸ", r"ã£ãŸ"), (r"ã‚‹ãŸ", r"ãŸ"),
    (r"ããŸ", r"ã„ãŸ"), (r"ããŸ", r"ã„ã "), (r"ã‚€ãŸ", r"ã‚“ã "),
    (r"ã†ã¾ã™", r"ã„ã¾ã™"), (r"ã¤ã¾ã™", r"ã¡ã¾ã™"), (r"ã‚‹ã¾ã™", r"ã¾ã™"),
]

class TextRequest(BaseModel):
    text: str
    noise_level: float = 0.5
    human_level: float = 0.5

@app.post("/humanize")
def humanize_text(req: TextRequest):
    text = req.text
    noise_lv = req.noise_level
    human_lv = req.human_level
    
    tokens = tokenizer_obj.tokenize(text, mode)
    result_buffer = ""
    
    for token in tokens:
        word = token.surface()
        if word in NOUN_DICT and random.random() < (human_lv + 0.1):
            word = random.choice(NOUN_DICT[word])
        if random.random() < (noise_lv * 0.05):
            word = random.choice(FILLERS) + word
        result_buffer += word
    
    processed_text = result_buffer
    
    # æ–‡æœ«èª¿æ•´
    sentences = processed_text.split("ã€‚")
    final_sentences = []
    for s in sentences:
        if not s: continue
        for pattern, candidates in ENDING_PATTERNS:
            if re.search(pattern, s) and random.random() < (human_lv + 0.2):
                s = re.sub(pattern, random.choice(candidates), s)
                break
        final_sentences.append(s)
        
    processed_text = "ã€‚".join(final_sentences)
    
    # æ–‡æ³•æ•´å½¢
    for pattern, replacement in GRAMMAR_FIXES:
        processed_text = re.sub(pattern, replacement, processed_text)
    
    if text.endswith("ã€‚") and not processed_text.endswith("ã€‚"):
        processed_text += "ã€‚"

    return {"result": processed_text}

@app.get("/")
def read_root():
    return {"status": "Ushiro-Brain V5 Stable"}
